{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import scorer, make_scorer, roc_auc_score, roc_curve\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import config\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from tqdm import tqdm_notebook, tqdm, tnrange\n",
    "tqdm.pandas()\n",
    "\n",
    "from numba import jit\n",
    "\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "import catboost\n",
    "\n",
    "from scipy.stats import gmean\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighted Mean encoding.\n",
    "# Usage:\n",
    "# X_tr_mean = mean_likelihood(X_tr, var, 'is_click')\n",
    "# mean_enc_var[val_idx] = X_val[var].map(X_tr_mean['enc'])\n",
    "def mean_likelihood(df, cat_var, target, alpha = 0.5):\n",
    "    P_c = df.groupby(cat_var)[target].transform('mean')\n",
    "    P_global = df[target].mean()\n",
    "    n_c = df.groupby(cat_var)[target].transform('count')\n",
    "    enc = (P_c*n_c + P_global*alpha)/(n_c + alpha)\n",
    "    temp = df[[cat_var]]\n",
    "    temp['enc'] = enc\n",
    "    return temp.groupby(cat_var).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label encoding.\n",
    "# Usage:\n",
    "# lb_dict, _ = label_enc(df_data[var])\n",
    "def label_enc(x):\n",
    "    values = x.unique()\n",
    "    lb_enc = defaultdict(np.int32)\n",
    "    for i, v in enumerate(values):\n",
    "        lb_enc[v] = i\n",
    "    return lb_enc, values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One hot encoding.\n",
    "# Returns numpy 2D array\n",
    "def one_hot_enc(x):\n",
    "    values = x.unique()\n",
    "    values_dict = {v:i for i, v in enumerate(values)}\n",
    "    one_hot_enc = np.zeros(shape = (len(x), len(values)))\n",
    "    for idx, v in enumerate(x):\n",
    "        one_hot_enc[idx, values_dict[v]] = 1\n",
    "    return one_hot_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adds mean encoded features to given train, test data frame using categorical variables and a target.\n",
    "# This is done using KFold data to prevent overfitting.\n",
    "# Returns train, test data frames with mean encoding columns 'mean_enc_'\n",
    "def add_mean_encoding(df_train, df_test, cat_vars, target, n_splits=10, random_state=1):\n",
    "    # getting mean encoding features\n",
    "    cvlist = list(KFold(n_splits = n_splits, random_state = random_state).split(df_train))\n",
    "    for var in cat_vars:\n",
    "        mean_enc_var = np.zeros(len(df_train))\n",
    "        for tr_idx, val_idx in cvlist:\n",
    "            X_tr, X_val = df_train.loc[tr_idx], df_train.loc[val_idx]\n",
    "            X_tr_mean = mean_likelihood(X_tr, var, target)\n",
    "            mean_enc_var[val_idx] = X_val[var].map(X_tr_mean['enc'])\n",
    "            df_train[f'mean_enc_{var}'] = mean_enc_var\n",
    "        df_train[f'mean_enc_{var}'] = df_train[f'mean_enc_{var}'].fillna(df_train[f'mean_enc_{var}'].mean())\n",
    "        df_test[f'mean_enc_{var}'] = df_test[var].map(mean_likelihood(df_train, \n",
    "                                                                        var, target)['enc'])\n",
    "        df_test[f'mean_enc_{var}'] = df_test[f'mean_enc_{var}'].fillna(df_train[f'mean_enc_{var}'].mean())\n",
    "        \n",
    "    \n",
    "    return df_train, df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Generating all possible pair of interactions between 2 pair of columns.\n",
    "##Then removing those columns with all 0s\n",
    "def add_pairwise_interactions(df):\n",
    "    combos = list(combinations(list(df.columns), 2))\n",
    "    colnames = list(df.columns)+['_'.join(x) for x in combos]\n",
    "    \n",
    "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
    "    df = poly.fit_transform(df)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.columns = colnames\n",
    "    \n",
    "    noint_indices = [i for i,x in enumerate(list((df==0).all())) if x]\n",
    "    df= df.drop(df.columns[noint_indices], axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test helper to combine train and test dataset.\n",
    "# mean encoding are added only on train and test separately.\n",
    "# Combining is needed so that columns could be normalized or new features are added.\n",
    "# Usage:\n",
    "# helper = TrainTestHelper()\n",
    "# train_test = helper.combine(train, test)\n",
    "# train, test = helper.split(train_test)\n",
    "class TrainTestHelper(object):\n",
    "    def __init__(self):\n",
    "        self.ntrain = None\n",
    "\n",
    "    def combine(self, train, test):\n",
    "        self.ntrain = train.shape[0]\n",
    "        if isinstance(train, np.ndarray):\n",
    "            return np.row_stack((train, test))\n",
    "        else:\n",
    "            return train.append(test).reset_index(drop=True)\n",
    "\n",
    "    def split(self, train_test):\n",
    "        if self.ntrain is None:\n",
    "            return None\n",
    "        if isinstance(train_test, np.ndarray):\n",
    "            train = train_test[:self.ntrain, :]\n",
    "            test = train_test[self.ntrain:, :]\n",
    "        else:\n",
    "            train = train_test.iloc[:self.ntrain, :].copy().reset_index(drop=True)\n",
    "            test = train_test.iloc[self.ntrain:, :].copy().reset_index(drop=True)\n",
    "        return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generates new features for a classification dataset.\n",
    "# If there are C class labels for target and k is set to N.\n",
    "# N*C features are generated. where each feature represents \n",
    "# the sum of distance from current instance to k nearest neighbors\n",
    "# of class C. More details in \n",
    "# http://davpinto.com/fastknn/articles/knn-extraction.html#understanding-the-knn-features\n",
    "# This technique was used in winner solution of \n",
    "# https://www.kaggle.com/c/otto-group-product-classification-challenge/forums/t/14335/1st-place-winner-solution-gilberto-titericz-stanislav-semenov\n",
    "# Usage:\n",
    "# newX, testX = knnExtract(X, y, test_X, k=1, holds = 5)\n",
    "# where newX, testX contains additional features for train and test data respectively.\n",
    "def _distance(a, b):\n",
    "    return np.linalg.norm(b - a)\n",
    "\n",
    "def _get_feat(data, X_train, y_train, class_index, k_index):\n",
    "    inclass_X = X_train[y_train == class_index]\n",
    "    distances = np.array([_distance(a, data) for a in inclass_X])\n",
    "    sorted_distances_index = np.argsort(distances)\n",
    "    nearest_index = list(sorted_distances_index[0: (k_index + 1)])\n",
    "    dist = np.sum(distances[nearest_index])\n",
    "    return dist\n",
    "\n",
    "def knnExtract(X, y, test_X=None, k = 1, holds = 5):\n",
    "    CLASS_NUM = len(set(y))\n",
    "    res = np.empty((len(X), CLASS_NUM * k))\n",
    "    kf = KFold(n_splits = holds,  shuffle = True)\n",
    "    \n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        features = np.empty([0, len(X_test)])\n",
    "        \n",
    "        for class_index in range(CLASS_NUM):\n",
    "            for k_index in range(k):\n",
    "                feat = np.array([np.apply_along_axis(_get_feat, 1, X_test, X_train, y_train, class_index, k_index)])\n",
    "                features = np.append(features, feat, axis = 0)\n",
    "        res[test_index] = features.T\n",
    "    \n",
    "    test_res = None\n",
    "    if test_X is not None:\n",
    "        test_res = np.empty((len(test_X), CLASS_NUM * k))\n",
    "        features = np.empty([0, len(test_X)])\n",
    "        for class_index in range(CLASS_NUM):\n",
    "            for k_index in range(k):\n",
    "                feat = np.array([np.apply_along_axis(_get_feat, 1, test_X, X, y, class_index, k_index)])\n",
    "                features = np.append(features, feat, axis = 0)\n",
    "        test_res = features.T\n",
    "\n",
    "    return res, test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
